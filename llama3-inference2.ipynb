{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-04T19:59:50.671251Z","iopub.status.busy":"2024-05-04T19:59:50.670423Z","iopub.status.idle":"2024-05-04T19:59:50.680079Z","shell.execute_reply":"2024-05-04T19:59:50.679089Z","shell.execute_reply.started":"2024-05-04T19:59:50.671220Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/customer-support/Cust_1k.json\n","/kaggle/input/customer-support-data/Cust_1k.json\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T19:59:53.991335Z","iopub.status.busy":"2024-05-04T19:59:53.990936Z","iopub.status.idle":"2024-05-04T20:00:14.206616Z","shell.execute_reply":"2024-05-04T20:00:14.205449Z","shell.execute_reply.started":"2024-05-04T19:59:53.991298Z"},"trusted":true},"outputs":[],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers trl==0.4.7"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T20:00:35.075036Z","iopub.status.busy":"2024-05-04T20:00:35.074652Z","iopub.status.idle":"2024-05-04T20:00:54.574792Z","shell.execute_reply":"2024-05-04T20:00:54.573991Z","shell.execute_reply.started":"2024-05-04T20:00:35.075001Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-04 20:00:44.789335: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-04 20:00:44.789466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-04 20:00:44.926207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import accelerate\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","import torch\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T20:00:54.576674Z","iopub.status.busy":"2024-05-04T20:00:54.576379Z","iopub.status.idle":"2024-05-04T20:00:56.212556Z","shell.execute_reply":"2024-05-04T20:00:56.211598Z","shell.execute_reply.started":"2024-05-04T20:00:54.576647Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_value_0 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n","!huggingface-cli login --token $secret_value_0\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T20:00:56.215217Z","iopub.status.busy":"2024-05-04T20:00:56.214298Z","iopub.status.idle":"2024-05-04T20:03:59.264662Z","shell.execute_reply":"2024-05-04T20:03:59.263862Z","shell.execute_reply.started":"2024-05-04T20:00:56.215176Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b8fd4bfa3264ad0a97aac61e1fe24fb","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/697 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8526b6cec2764af6b5080a9866e9db51","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27d2f20a4f0d4340b91216c2814febd4","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49e67f25650d4bc0a63dba479c88a4cf","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b000fee30094708a3513f0c79a361e2","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4436454427904ce8ab89b05bb961039c","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f0a1d5bfdf040bca08c507d4327118e","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f58193e1e71045d4ac6cf1674c57226a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4051305626a44d01b9ff0c0858d184c7","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import accelerate\n","#model = AutoModelForCausalLM.from_pretrained(\"PygmalionAI/pygmalion-6b\", low_cpu_mem_usage=True, device_map='auto', torch_dtype=torch.float16)\n","model = AutoModelForCausalLM.from_pretrained(\"uday91/Meta-Llama-3-8B-customer-support-finetune\", device_map='auto', torch_dtype=torch.float16)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T20:04:07.195060Z","iopub.status.busy":"2024-05-04T20:04:07.194308Z","iopub.status.idle":"2024-05-04T20:04:08.449973Z","shell.execute_reply":"2024-05-04T20:04:08.449028Z","shell.execute_reply.started":"2024-05-04T20:04:07.195026Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cded03b4d4a472b9f04a21c105f0bc7","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3fc92c7cdba46199e5b428e8703fea8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d9d4bd4bf8047c9a0ac31b0d8278be7","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"uday91/Meta-Llama-3-8B-customer-support-finetune\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T20:04:10.418762Z","iopub.status.busy":"2024-05-04T20:04:10.418094Z","iopub.status.idle":"2024-05-04T20:04:10.423087Z","shell.execute_reply":"2024-05-04T20:04:10.422105Z","shell.execute_reply.started":"2024-05-04T20:04:10.418728Z"},"trusted":true},"outputs":[],"source":["\n","# # Example prompt\n","# prompt = \"help to see when my purchase is going to arrive\"\n","\n","# # Generate text\n","# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n","# output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n","# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","# print(\"Generated text:\")\n","# print(generated_text)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T20:04:12.011704Z","iopub.status.busy":"2024-05-04T20:04:12.011052Z","iopub.status.idle":"2024-05-04T20:04:12.340993Z","shell.execute_reply":"2024-05-04T20:04:12.339901Z","shell.execute_reply.started":"2024-05-04T20:04:12.011668Z"},"trusted":true},"outputs":[],"source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T20:04:13.711130Z","iopub.status.busy":"2024-05-04T20:04:13.710429Z","iopub.status.idle":"2024-05-04T20:04:49.945224Z","shell.execute_reply":"2024-05-04T20:04:49.944165Z","shell.execute_reply.started":"2024-05-04T20:04:13.711100Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] help to see when my purchase is going to arrive? [/INST] I understand your curiosity about the estimated arrival time of your purchase. To provide you with accurate information, could you please provide me with the {{Order Number}} or {{Tracking Number}}? With that, I'll be able to track your order and provide you with the most up-to-date information regarding its delivery. Thank you for your cooperation! </s> How can I assist you further? \" </s> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </inst> </\n"]}],"source":["\n","# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"help to see when my purchase is going to arrive?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T20:05:41.888195Z","iopub.status.busy":"2024-05-04T20:05:41.887274Z","iopub.status.idle":"2024-05-04T20:06:15.572733Z","shell.execute_reply":"2024-05-04T20:06:15.571720Z","shell.execute_reply.started":"2024-05-04T20:05:41.888156Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] how to edit my delivery address  [/INST] I appreciate your message to me about editing your delivery address. I'm here to assist you with this process. To edit your delivery address, you can follow these steps:\n","\n","1. Log in to your account on our website.\n","2. Go to the \"My Account\" or \"Profile\" section.\n","3. Look for the \"Shipping Addresses\" or similar option.\n","4. Click on the \"Edit\" or \"Manage\" button next to the address you want to update.\n","5. Make the necessary changes to your delivery address.\n","6. Save the updated information.\n","\n","If you encounter any difficulties or have any questions during the process, please don't hesitate to let me know. I'm here to help you every step of the way. Thank you for trusting us with your delivery address details. We value your feedback and appreciate your patience as we assist you with this important update. </s> How can I assist you further? \" </s> Thank you for reaching out to us! I understand that you would like to edit your delivery address. To do so, please follow these steps:\n","\n","1. Log in to your account on our website.\n","2. Navigate to the \"My Account\" or \"Profile\" section.\n","3. Look for the \"Shipping Addresses\" or similar option.\n","4. Click on the \"Edit\" or \"Manage\" button next to the address you want to update.\n","5. Make the necessary changes to your delivery address.\n","6. Save the updated information.\n","\n","If you need any further assistance or encounter any difficulties during the process, please don't hesitate to let me know. We're here to help you with your delivery address edit and ensure a smooth experience for you. Let me know if there's anything else I can assist you with. </s> How can I help you today? \" </s> I'm here to assist you with editing your delivery address. To get started, please log in to your account on our website. Once you're logged in, navigate to the \"My Account\" or \"Profile\" section. Look for the \"Shipping Addresses\" or similar option. You should see a list of your existing addresses. Find the address you would like to edit and click on the \"Edit\" or \"Manage\" button next to it. This will open a form where you can make the necessary changes to your delivery address. Once you're done, save the updated information. If you\n"]}],"source":["\n","# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"how to edit my delivery address \"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T14:34:19.387016Z","iopub.status.busy":"2024-04-29T14:34:19.385956Z","iopub.status.idle":"2024-04-29T14:35:29.176435Z","shell.execute_reply":"2024-04-29T14:35:29.175381Z","shell.execute_reply.started":"2024-04-29T14:34:19.386981Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] how to check the payment modalities? [/INST] I understand your need to check the payment modalities available to you. Here's some information about the different payment options:\n","\n","1. **Credit/Debit Card:** You can use a credit or debit card to make payments securely online.\n","2. **PayPal:** If you have a PayPal account, you can use it to pay for your purchases.\n","3. **Bank Transfer:** You can also make payments through your bank account using the bank transfer option.\n","4. **Cash on Delivery:** If you prefer, you can choose the cash on delivery option and pay in cash when your order arrives.\n","\n","If you have any specific questions about a particular payment method or need assistance with any of these options, please let me know. I'm here to help! </s> How can I assist you further? \" Check out our payment modalities at https://www.website.com/payment-modalities. If you have any questions or need further assistance, feel free to reach out. I'm here to help! \" How can I assist you further? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If you have any questions or need additional information, please let me know. How can I help you specifically? \" </s> I'm here to make sure you have a seamless experience. If\n"]}],"source":["\n","# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"how to check the payment modalities?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T14:44:31.165807Z","iopub.status.busy":"2024-04-29T14:44:31.165067Z","iopub.status.idle":"2024-04-29T14:45:40.750637Z","shell.execute_reply":"2024-04-29T14:45:40.749560Z","shell.execute_reply.started":"2024-04-29T14:44:31.165773Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] Considering leaving a review for a service I've experienced. [/INST] I appreciate your interest in leaving a review for our service. Your feedback is incredibly valuable to us as it helps us understand how we can improve and provide better experiences for our customers. To leave a review, please visit our website and navigate to the \"Reviews\" section. There, you will find a form where you can share your thoughts and experiences. We encourage you to be as detailed as possible, so that we can gain a deeper understanding of your experience. Thank you for taking the time to share your feedback with us, and we look forward to hearing from you! </s> I'm here to assist you with leaving a review for our service. To get started, please provide me with the details of your experience, including the specific service you would like to review and any feedback you have. This information will help me guide you through the process and ensure that your review is accurate and informative. Thank you for your contribution! </s> I'm excited to hear about your experience with our service. To leave a review, please visit our website and navigate to the \"Reviews\" section. There, you will find a form where you can share your thoughts and experiences. We appreciate your feedback, as it helps us understand how we can improve and provide better experiences for our customers. If you have any questions or need assistance with the review process, feel free to ask. Your input is invaluable to us! </s> I'm here to guide you through the process of leaving a review for our service. To get started, please let me know which specific service you would like to review. Once I have that information, I can provide you with the steps and guidance you need to share your feedback. Your input is important to us, and we value your perspective. We're here to support you every step of the way. </s> I'm here to assist you with leaving a review for our service. To get started, please provide me with the details of your experience, including the specific service you would like to review and any feedback you have. This information will help me guide you through the process and ensure that your review is accurate and informative. Thank you for your contribution! </s> I'm excited to hear about your experience with our service. To leave a review, please visit our website and navigate to the \"Reviews\" section. There, you will find a form where you can share your thoughts and experiences. We appreciate your feedback, as it helps us understand how we can improve and provide better experiences for our customers. If you have any questions or need assistance with the review process, feel free to ask. Your input is invaluable to us! </s> I'm here to guide you through the process of leaving a review for our service. To get started, please let me know which specific service you would like to review. Once I have that information, I can provide you with the steps and guidance you need to share your feedback. Your input is important to us, and we value your perspective. We're here to support you every step of the way. </s> I'm here to assist you with leaving a review for our service. To get started, please provide me with the details of your experience, including the specific service you would like to review and any feedback you have. This information will help me guide you through the process and ensure that your review is accurate and informative. Thank you for your contribution! </s> I'm excited to hear about your experience with our service. To leave a review, please visit our website and navigate to the \"Reviews\" section. There, you will find a form where you can share your thoughts and experiences. We appreciate your feedback, as it helps us understand how we can improve and provide better experiences for our customers. If you have any questions or need assistance with the review process, feel free to ask. Your input is invaluable to us! </s> I'm here to guide you through the process of leaving a review for our service. To get started, please let me know which specific service you would like to review. Once I have that information, I can provide you with the steps and guidance you need to share your feedback. Your input is important to us, and we value your perspective. We're here to support you every step of the way. </s> I'm here to assist you with leaving a review for our service. To get started, please provide me with the details of your experience, including the specific service you would like to review and any feedback you have. This information will help me guide you through the process and ensure that your review is accurate and informative. Thank you for your contribution! </s> I'm excited to hear about your experience with our service. To leave a review, please visit our website and navigate to the \"Reviews\" section. There, you will find a form\n"]}],"source":["\n","# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"Considering leaving a review for a service I've experienced.\"\n","#pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T14:48:07.739832Z","iopub.status.busy":"2024-04-29T14:48:07.739411Z","iopub.status.idle":"2024-04-29T14:49:17.379041Z","shell.execute_reply":"2024-04-29T14:49:17.377917Z","shell.execute_reply.started":"2024-04-29T14:48:07.739798Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] Considering leaving a review for a service I've experienced. [/INST] I'm delighted to assist you with leaving a review for a service you've experienced. Your feedback is highly valuable to us, as it helps us improve our services and provide better experiences for our customers. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </INST> \"Consider leaving a review\" is a phrase that indicates a person is contemplating leaving a review for a service they have experienced. To provide the best assistance, I would appreciate it if you could provide me with more details about the specific service you are thinking of reviewing. This will enable me to guide you through the process and ensure that your review is accurately reflected. Please let me know which service you are considering leaving a review for, and I will be more than happy to assist you further. </s> </Consider leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to review? This will enable me to guide you through the process and ensure that your review is accurately reflected. </s> </I'm here to help you with leaving a review\" I'm here to help you with leaving a review for the service you've experienced. Could you please provide me with more details about the specific service you would like to\n"]}],"source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","prompt = \"Considering leaving a review for a service I've experienced.\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1000,temperature =0.5)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T21:42:40.712157Z","iopub.status.busy":"2024-04-29T21:42:40.711408Z","iopub.status.idle":"2024-04-29T21:45:16.798622Z","shell.execute_reply":"2024-04-29T21:45:16.797896Z","shell.execute_reply.started":"2024-04-29T21:42:40.712128Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a217578234f94f3f9a53f46fc2646dca","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/697 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"657328051e0a4dc7b18482380191b33a","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acf8521402ff45318632280e7ed5edeb","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72321b08bceb406abfa555ce7dae0e37","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c21f83a125e49e6b5530d2248587947","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e0f9ec0c3274e82b51bb5ce06960954","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4324eb17efed42479ca6389dd1f9dc8b","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a81e849b21594fa8a7b7d0a3e42ab3d1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0228daa3d5384c3d84aa7057c8b77aa9","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n","hf_model_id = \"uday91/Meta-Llama-3-8B-customer-support-finetune\"\n","model = AutoModelForCausalLM.from_pretrained(hf_model_id,device_map='auto', torch_dtype=torch.float16)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T22:00:27.364681Z","iopub.status.busy":"2024-04-29T22:00:27.363959Z","iopub.status.idle":"2024-04-29T22:00:27.922152Z","shell.execute_reply":"2024-04-29T22:00:27.921297Z","shell.execute_reply.started":"2024-04-29T22:00:27.364645Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(hf_model_id, legacy=False)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T15:09:01.087485Z","iopub.status.busy":"2024-04-29T15:09:01.086652Z","iopub.status.idle":"2024-04-29T15:09:24.416447Z","shell.execute_reply":"2024-04-29T15:09:24.415231Z","shell.execute_reply.started":"2024-04-29T15:09:01.087451Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] Considering leaving a review for a service I've experienced. [/INST] I'm aware that you're considering leaving a review for a service you've experienced. Your feedback is valuable to us, and we appreciate your willingness to share your thoughts. If you have any specific questions or need assistance with the review process, please let me know. I'm here to help you every step of the way. </s> \" I'm aware that you're considering leaving a review for a service you've experienced. Your feedback is valuable to us, and we appreciate your willingness to share your thoughts. If you have any specific questions or need assistance with the review process, please let me know. I'm here to help you every step of the way. \" </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"]}],"source":["generation_config, unused_kwargs = GenerationConfig.from_pretrained(\n","    hf_model_id, max_new_tokens=300, temperature=0.3, return_unused_kwargs=True\n",")\n","model.generation_config = generation_config\n","pipe = pipeline(\n","    \"text-generation\", \n","    model=model, \n","    tokenizer=tokenizer,\n","\n",")\n","prompt = \"Considering leaving a review for a service I've experienced.\"\n","\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T15:11:41.679925Z","iopub.status.busy":"2024-04-29T15:11:41.679147Z","iopub.status.idle":"2024-04-29T15:12:16.431629Z","shell.execute_reply":"2024-04-29T15:12:16.430527Z","shell.execute_reply.started":"2024-04-29T15:11:41.679889Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] Considering leaving a review for a service I've experienced. [/INST] I'm glad you're considering leaving a review for our service. Your feedback is incredibly valuable to us as it helps us understand how we can improve and provide you with the best experience possible. If you have any specific details about the service you would like to share, please feel free to elaborate. Your honest thoughts and opinions will help us make positive changes and ensure that we continue to meet your expectations. Thank you for taking the time to share your experience with us. </s> If you have any other questions or need further assistance, please don't hesitate to let me know. I'm here to help you every step of the way. \" </s> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST> </INST\n"]}],"source":["generation_config, unused_kwargs = GenerationConfig.from_pretrained(\n","    hf_model_id, max_new_tokens=500, temperature=0.5, return_unused_kwargs=True\n",")\n","model.generation_config = generation_config\n","pipe = pipeline(\n","    \"text-generation\", \n","    model=model, \n","    tokenizer=tokenizer,\n","\n",")\n","prompt = \"Considering leaving a review for a service I've experienced.\"\n","\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T15:13:44.288244Z","iopub.status.busy":"2024-04-29T15:13:44.287787Z","iopub.status.idle":"2024-04-29T15:14:19.123408Z","shell.execute_reply":"2024-04-29T15:14:19.122307Z","shell.execute_reply.started":"2024-04-29T15:13:44.288204Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] Considering leaving a review for a service I've experienced. [/INST] I'm so glad you're considering leaving a review for our service! Your feedback is incredibly valuable to us as it helps us improve and provide better experiences for our customers. If you could share your thoughts and experiences with our service, it would be greatly appreciated. You can leave a review on our website, social media platforms, or through our customer support team. We're here to listen and learn from your experiences. Thank you for your contribution! </s> \" I'm so glad you're considering leaving a review for our service! Your feedback is incredibly valuable to us as it helps us improve and provide better experiences for our customers. If you could share your thoughts and experiences with our service, it would be greatly appreciated. You can leave a review on our website, social media platforms, or through our customer support team. We're here to listen and learn from your experiences. Thank you for your contribution! \" </s> </code> This code uses the {{Customer Experience}} and {{Review}} tags to identify the intent of the customer and provide a personalized response. The code includes a warm and encouraging tone, expressing gratitude for the customer's consideration and emphasizing the importance of their feedback. The customer is provided with multiple options for leaving a review, including the website, social media platforms, and customer support team. By offering these options, the code ensures that the customer has a convenient and accessible way to share their experiences. Additionally, the code includes a call-to-action, encouraging the customer to leave a review and highlighting the benefits of their contribution. Overall, this code demonstrates a customer-centric approach, prioritizing the customer's needs and ensuring a positive and rewarding experience. </s> </code> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div>\n"]}],"source":["generation_config, unused_kwargs = GenerationConfig.from_pretrained(\n","    hf_model_id, max_new_tokens=500, temperature=0.5, return_unused_kwargs=True\n",")\n","model.generation_config = generation_config\n","pipe = pipeline(\n","    \"text-generation\", \n","    model=model, \n","    tokenizer=tokenizer,\n","\n",")\n","prompt = \"Considering leaving a review for a service I've experienced.\"\n","\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T15:16:15.146090Z","iopub.status.busy":"2024-04-29T15:16:15.145077Z","iopub.status.idle":"2024-04-29T15:16:50.185290Z","shell.execute_reply":"2024-04-29T15:16:50.184222Z","shell.execute_reply.started":"2024-04-29T15:16:15.146051Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] Considering leaving a review for a service I've experienced. [/INST] I'm here to assist you with leaving a review for the service you've experienced. Your feedback is highly valued, and it helps us improve our services and meet your expectations. Could you please provide me with more details about the service you would like to review? This will allow me to guide you through the process and ensure that your review is accurate and informative. </s> \"I'm here to assist you with leaving a review for the service you've experienced. Your feedback is highly valued, and it helps us improve our services and meet your expectations. Could you please provide me with more details about the service you would like to review? This will allow me to guide you through the process and ensure that your review is accurate and informative. </s> \"I'm here to assist you with leaving a review for the service you've experienced. Your feedback is highly valued, and it helps us improve our services and meet your expectations. Could you please provide me with more details about the service you would like to review? This will allow me to guide you through the process and ensure that your review is accurate and informative. </s> \"I'm here to assist you with leaving a review for the service you've experienced. Your feedback is highly valued, and it helps us improve our services and meet your expectations. Could you please provide me with more details about the service you would like to review? This will allow me to guide you through the process and ensure that your review is accurate and informative. </s> \"I'm here to assist you with leaving a review for the service you've experienced. Your feedback is highly valued, and it helps us improve our services and meet your expectations. Could you please provide me with more details about the service you would like to review? This will allow me to guide you through the process and ensure that your review is accurate and informative. </s> \"I'm here to assist you with leaving a review for the service you've experienced. Your feedback is highly valued, and it helps us improve our services and meet your expectations. Could you please provide me with more details about the service you would like to review? This will allow me to guide you through the process and ensure that your review is accurate and informative. </s> \"I'm here to assist you with leaving a review for the service you've experienced. Your feedback is highly valued, and it helps us improve our services and meet your expectations. Could you please provide me with more details about\n"]}],"source":["generation_config, unused_kwargs = GenerationConfig.from_pretrained(\n","    hf_model_id, max_new_tokens=500, temperature=0.5,return_unused_kwargs=True\n",")\n","model.generation_config = generation_config\n","pipe = pipeline(\n","    \"text-generation\", \n","    model=model, \n","    tokenizer=tokenizer,\n","\n",")\n","prompt = \"Considering leaving a review for a service I've experienced.\"\n","\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T22:00:58.677963Z","iopub.status.busy":"2024-04-29T22:00:58.677272Z","iopub.status.idle":"2024-04-29T22:00:59.496182Z","shell.execute_reply":"2024-04-29T22:00:59.494692Z","shell.execute_reply.started":"2024-04-29T22:00:58.677923Z"},"trusted":true},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 119.06 MiB is free. Process 2368 has 14.63 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 993.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      8\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi want to write an review about a service\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<s>[INST] \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprompt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m [/INST]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:240\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1206\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1199\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1200\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         )\n\u001b[1;32m   1204\u001b[0m     )\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1213\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1212\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1213\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1111\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1112\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:327\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1575\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1568\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1569\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1570\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1572\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1575\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1593\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1594\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1600\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2697\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2694\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2696\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2697\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2705\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1215\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1215\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1218\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:160\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(old_forward)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 160\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:286\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map[name]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8:\n\u001b[1;32m    285\u001b[0m                 fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map[name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m--> 286\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    291\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    292\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:298\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    296\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 298\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 119.06 MiB is free. Process 2368 has 14.63 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 993.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["generation_config, unused_kwargs = GenerationConfig.from_pretrained(\n","    hf_model_id, max_new_tokens=500, temperature=0.5, return_unused_kwargs=True\n",")\n","model.generation_config = generation_config\n","pipe = pipeline(\n","    \"text-generation\", \n","    model=model, \n","    tokenizer=tokenizer,\n","\n",")\n","prompt = \"i want to write an review about a service\"\n","\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4765776,"sourceId":8075641,"sourceType":"datasetVersion"},{"datasetId":4797802,"sourceId":8119856,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
